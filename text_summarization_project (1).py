# -*- coding: utf-8 -*-
"""Text Summarization Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ScqZayeABbiu_5YjtHsqNUy10E-v4P1n
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import pickle
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.optimizers import SGD, Adam
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns
# from keras.models import Sequential|
import random

# %matplotlib inline

#np.random.seed(11)

model = Sequential()
model.add(Dense(10, input_shape=(2,)))
model.add(Activation('sigmoid'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.5, momentum=0.01), 
              metrics=['accuracy'])

model.summary()

tensorboard_callback = TensorBoard(log_dir="./logs", write_graph=True, histogram_freq=1)

from keras.callbacks import EarlyStopping
es = EarlyStopping(patience=100, monitor='val_accuracy', restore_best_weights=True)

def split_dataset(path):
    dataset = open(path).read().split('\n')

    dataset_len = len(dataset)
    print("There are %s headline-article pairs" % dataset_len)

    #  shuffle data
    ##random.sample(population, k, *, counts=None)
    # random.shuffle(x[, random])
    dataset = np.array(random.shuffle(dataset, len(dataset)))

    # Split dataset to 70% training, 20% evaluation and 10% testing.


    v_point = int(nb_samples*(1-valid_split-test_split))
    t_point = int(nb_samples*(1-test_split))
    X_train = X[0:v_point]
    Y_train = Y[0:v_point]
    X_valid = X[v_point:t_point]
    Y_valid = Y[v_point:t_point]
    X_test  = X[t_point:]
    Y_test  = Y[t_point:]


   # train_size = int(dataset_len*0.7)
    #eval_size = int(dataset_len*0.2)
   # $train, eval, test = dataset[0:train_size], dataset[train_size:train_size+eval_size], dataset[train_size+eval_size:]
   # return train, eval, test

#shuffle data functions
##random.sample(population, k, *, counts=None)
##random.shuffle(x[, random])

#retrieve data by path
articles_path = '/content/sample_data/article.txt'
headlines_path = '/content/sample_data/headline.txt'
#X_train, X_eval, X_test = split_dataset(headlines_path)
#Y_train, Y_eval, Y_test = split_dataset(articles_path)
  # deviding sets
    
Articles_dataset = open(articles_path).read().split('\n')
Articles_dataset_len = len(Articles_dataset)

# random.shuffle(x[, random])
Articles_dataset = np.array(random.sample(Articles_dataset, len(Articles_dataset)))
#print(len(Articles_dataset))


Headlines_dataset = open(headlines_path).read().split('\n')
Headlines_dataset_len = len(Headlines_dataset)

# random.shuffle(x[, random])
Headlines_dataset = np.array(random.sample(Headlines_dataset, Headlines_dataset_len))


print("There are %s headline-article pairs" % Headlines_dataset_len)

valid_split = 0.2
test_split  = 0.1   


# Split dataset to 90% training, 5% evaluation and 5% testing.
X_train_size = int(Headlines_dataset_len*0.9)
X_valid_size = int(Headlines_dataset_len*0.05)
X_train, X_valid, X_test = Headlines_dataset[0:X_train_size], Headlines_dataset[X_train_size:X_train_size+X_valid_size], Headlines_dataset[X_train_size+X_valid_size:]


Y_train_size = int(Articles_dataset_len*0.9)
Y_valid_size = int(Articles_dataset_len*0.05)
Y_train, Y_valid, Y_test = Articles_dataset[0:Y_train_size], Articles_dataset[Y_train_size:Y_train_size+Y_valid_size], Articles_dataset[Y_train_size+Y_valid_size:]

#v_point = int(Articles_dataset_len*(1-valid_split-test_split))
#t_point = int(Articles_dataset_len*(1-test_split))
#X_train = X[0:v_point]
#Y_train = Y[0:v_point]
#X_valid = X[v_point:t_point]
#Y_valid = Y[v_point:t_point]
#X_test  = X[t_point:]
#Y_test  = Y[t_point:]


#scaler = preprocessing.StandardScaler() #(((X-np.mean(X))/np.std(X)))
#X_train = scaler.fit_transform(train)
#X_valid = scaler.transform(valid)
#X_test  = scaler.transform(test)
 
  
#print("TRAINING MEAN:",np.mean(X_train),", VARIANCE:",np.std(X_train) ) 
#print("VALIDATION MEAN:",np.mean(X_valid),", VARIANCE:",np.std(X_valid) )
#print("TEST MEAN:",np.mean(X_test),", VARIANCE:",np.std(X_test) )
    #write_dataset(X_train, X_valid, X_test, Y_train, Y_valid, Y_test)

model.fit(X_train, Y_train,
          batch_size=32, # 700 / 32 ~= 22
          epochs = 100000000,
          verbose = 2,  #0,1,2
          validation_data = (X_valid,Y_valid),
          callbacks = [tensorboard_callback, es],
          shuffle = False
          )





"""# New Section"""